# -*- coding: utf-8 -*-
"""FDA_T1_CREDITRISK3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Aob3heP50XLRxeUBE2F9k4itpXIBIDOj

#Librerias
"""

import os
import sys
import glob

import numpy as np
import pandas as pd

import matplotlib.pylab as plt
from matplotlib_venn import venn2
import seaborn as sns

from tqdm import tqdm
from itertools import cycle

from sklearn import metrics
from sklearn import model_selection
from sklearn import preprocessing
from sklearn import linear_model
from sklearn import feature_selection

import lightgbm as lgb
import xgboost as xgb
import catboost as cat

import optbinning

import io

from scipy import stats

from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score

#from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency

"""# Lectura base de datos"""


data=pd.read_csv("credit_risk_dataset.csv")

"""# Pre-procesamiento"""

df = data.dropna(subset=['person_emp_length'])

"""por el momento la solucion para los valores faltantes de person_emp_lenght es borrar los registros que tengan ese valor en nulo.
pero se debe analizar la significancia de esta variable para nuestro modelo ya que la primera opcion seria eliminar la columna, como segunda opcion imputacion con estadisticos y por ultimo la eliminacion de registros

todavia hay 3048 nulos para esta columna, por lo cual no es viable eliminar registros, representan casi el 10% de los datos

ahora para la variable, "loan_int_rate"
"""

df2 = data.dropna(subset=['loan_int_rate'])

#se toma las edades menores a 100
mascara_age = df2['person_age'] <= 100
df3 = df2[mascara_age]

mascara = df3['person_age'] > df3['person_emp_length']
df4 = df3[mascara]

"""# Regresion logistica"""

# explore the unique values in loan_status column
df4['loan_status'].value_counts(normalize = True)

# split data into 80/20 while keeping the distribution of bad loans in test set same as that in the pre-split dataset
X = df4.drop('loan_status', axis = 1)
y = df4['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,
                                                    random_state = 42, stratify = y)

# hard copy the X datasets to avoid Pandas' SetttingWithCopyWarning when we play around with this data later on.
# this is currently an open issue between Pandas and Scikit-Learn teams
X_train, X_test = X_train.copy(), X_test.copy()

#Se eliminan las columnas debido al IV
X_train2 = X_train.drop(['person_age','cb_person_cred_hist_length'], axis = 1)
cols=[col for col in X_train2]

X_test2 = X_test.drop(['person_age','cb_person_cred_hist_length'], axis = 1)

# first divide training data into categorical and numerical subsets
X_train_cat = X_train.select_dtypes(include = 'object').copy()
X_train_num = X_train.select_dtypes(include = 'number').copy()
X_test_num = X_test.select_dtypes(include = 'number').copy()

"""4 variables categoricas y 7 variables numericas

Significancia para las variables categoricas segun prueba chi2

Significancia para las variables numericas segun ANOVA
"""

# function to create dummy variables
def dummy_creation(df, columns_list):
    df_dummies = []
    for col in columns_list:
        df_dummies.append(pd.get_dummies(df[col], prefix = col, prefix_sep = ':'))
    df_dummies = pd.concat(df_dummies, axis = 1)
    df = pd.concat([df, df_dummies], axis = 1)
    return df

# apply to our final four categorical variables
X_train = dummy_creation(X_train, ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])
X_test = dummy_creation(X_test, ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])
# reindex the dummied test set variables to make sure all the feature columns in the training set are also available in the test set
X_test = X_test.reindex(labels=X_train.columns, axis=1, fill_value=0)

X_train = X_train.drop(['person_age','cb_person_cred_hist_length'], axis = 1)

# binning process

# selection_criteria = {
#     "iv": {"min": 0.02, "max": 1},
#     "quality_score": {"min": 0.01}
# }

binning_process = optbinning.BinningProcess(
    variable_names=cols,
    #selection_criteria=selection_criteria,
    categorical_variables=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
)

# estimator
#estimator = linear_model.LogisticRegression()
estimator = linear_model.LogisticRegression(max_iter=1000, class_weight = 'balanced')

#pipeline = Pipeline(steps=[('woe', binning_process), ('model', estimator)])

# define cross-validation criteria. RepeatedStratifiedKFold automatially takes care of the class imbalance while splitting
#cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)

# fit and evaluate the logistic regression pipeline with cross-validation as defined in cv
#scores = cross_val_score(pipeline, X_train, y_train, scoring = 'roc_auc', cv = cv)
#AUROC = np.mean(scores)
#GINI = AUROC * 2 - 1

# print the mean AUROC score and Gini
#print('Mean AUROC: %.4f' % (AUROC))
#print('Gini: %.4f' % (GINI))

# scorecard
scorecard = optbinning.Scorecard(
    binning_process=binning_process,
    estimator=estimator,
    scaling_method="min_max",
    scaling_method_params={"min": 350, "max": 850},
    #scaling_method = "pdo_odds",
    #scaling_method_params = {"pdo": 20, "odds": 50, "scorecard_points": 100},
    #intercept_based=True,
    #reverse_scorecard=True
)

# model fitting
scorecard.fit(X_train, y_train)

# scorecard table
scorecard_df = scorecard.table(style="detailed")
scorecard_df.query("Variable == 'cb_person_default_on_file'")

X_train2['score'] = scorecard.score(X_train)
X_test2['score'] = scorecard.score(X_test)

X_test2

import pickle

with open('modelo_scorecard2.pkl', 'wb') as archivo_pkl:
    pickle.dump(scorecard, archivo_pkl)

obsc = cols

personincome = 69900
loanamount = 5050
percentincome = loanamount/personincome

vperson_income = personincome
vperson_home_ownership = 'MORTGAGE'
vperson_emp_length = 9.0
vloan_intent = 'DEBTCONSOLIDATION'
vloan_grade = 'A'
vloan_amnt = loanamount
vloan_int_rate = 5.42
vloan_percent_income = percentincome
cb_person_default_on_file = 'N'

data = [vperson_income, vperson_home_ownership, vperson_emp_length, vloan_intent, vloan_grade,vloan_amnt,vloan_int_rate,vloan_percent_income,cb_person_default_on_file]

obsdf = pd.DataFrame([data],columns = obsc)

score = scorecard.score(obsdf)

print(score)
